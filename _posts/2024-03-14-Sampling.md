---
layout: post
categories: OT@Berlin
---

## Sampling, particle system and neural network

Anna Korba (Sampling through Optimization of Discrepancies)

The task of sampling is to find the target measure $\mu^*$ from information of unnormalised density or discrete approximation.

The quality is given by the difference : 
$|\frac{1}{n} \sum_{i=1}^n f(x_i) - \int f(x)\mathrm{d} \mu(x)|$.

Reference [Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm, Liu, Wang 2016](https://arxiv.org/abs/1608.04471)

The optimisation of neural network can be mapped to approximating $\mu^{*}$ in single layer neural network with infinite width.
![image](https://github.com/solomon-lam/solomon-lam.github.io/assets/43318214/b6913bfa-e8bd-43ca-b9c6-f9d4a062ee47)

As an optimisation problem, we can use gradient flow with respect to Wasserstein metric over probability measure on $\mathbb{R}^d$. To implement the gradient flow, particles are used to approximate the flow of measure.
![image](https://github.com/solomon-lam/solomon-lam.github.io/assets/43318214/b34368f2-5269-4b44-bc8f-cf690095dd61)

As gradient descent is important in machine learning, it is not surprising that gradient flow enters the picture. Particle systems enter as an approximation of the gradient flow of measure. The metric of probability measures are known as divergence in this setting. There are different choices such as KL divergence (relative entropy), chi-divergence, MMD (maximum mean divergence). MMD is defined as the supremum over the functions in a reproducing kernel Hilbert space (RKHS). The difference of the integration over the two measures defined the metric known as MMD. She is proposing a new metric as a convex combination of MMD and chi-divergence
known as DMMD. In her talk, she tries to convince us this new metric is useful for the discrete case.


Jiaxin Shi (Stein’s Method for Modern Machine Learning: From Gradient Estimation to Generative Modeling)
This talk uses the Stein method. More precisely to construct a discrete state Markov process so that the stationary distribution is a given distribution. He is interested in the generator of this process and this will be the Stein operator in his application.

![Stein operator](https://github.com/solomon-lam/solomon-lam.github.io/assets/43318214/772565b0-b0d5-4de0-84e6-8145bda8fa42)

Gradient estimation is the problem to calculate
$$
\nabla_{\eta} \mathbb{E}_{q_{\eta}}(f(x))
$$

where 
$q_{\eta}$ 
is interpreted as a language model to predict the next token, $f$ is the reward model to evaluate the outcome of the prediction from the language model. It is then used to improve the model during a training process.

Reference [Measuring sample quality with Stein’s method, Jackson Gorham, Lester Mackey](https://arxiv.org/pdf/1611.06972.pdf)
![Discrete Stein operator from Markov process](https://github.com/solomon-lam/solomon-lam.github.io/assets/43318214/b32c5e6b-559c-48d0-b337-1c72cc218399)

[Gradient estimation with discrete Stein operators](https://proceedings.neurips.cc/paper_files/paper/2022/file/a5a5b0ff87c59172a13342d428b1e033-Paper-Conference.pdf)

Another part of his talk was about blind source separation, e.g. separating indvidual voices from a noisy room, score matching, score network to diffusion model. Diffusion model was behind the explosive growth in image generative model in recent days.

Oliver Tse: [Variational Acceleration Methods in the Space of Probability Measures](https://arxiv.org/abs/2310.04006)
Acceleration of convergence of gradient flow like equation?

Check his paper for more.

